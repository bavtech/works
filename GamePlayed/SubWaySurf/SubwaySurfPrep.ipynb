{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c690a39c-2dda-40e6-944d-704619b89324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from torch.nn import Conv2d, Linear, LazyLinear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import warnings\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84970db-65ff-4541-9637-2cc60d3cf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP EVERY CONV WITH A RELU LAYER\n",
    "class SubWayNet(nn.Module):\n",
    "\n",
    "    def __init__(self,  num_classes=5):\n",
    "\n",
    "        super(SubWayNet,self).__init__()\n",
    "\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,stride=1)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_layer3 =  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,stride=1)\n",
    "        self.conv_layer4 =  nn.Conv2d(in_channels=128, out_channels=256,kernel_size=3)\n",
    "\n",
    "        self.max_pool2 =  nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "\n",
    "        # self.fc1 =  LazyLinear(out_features=64)\n",
    "        self.fc1 = nn.Linear(56*120*256,64)\n",
    "        self.relu1 =  nn.ReLU()\n",
    "        self.fc2 =  nn.Linear(64, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out =  self.conv_layer1(x) \n",
    "        out = self.conv_layer2(out) \n",
    "        out =  self.max_pool1(out)\n",
    "\n",
    "        out =  self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out =  self.max_pool2(out) \n",
    "        # print(out.shape)\n",
    "        out =  out.view(-1,56*120*256)\n",
    "\n",
    "        out =  self.fc1(out)\n",
    "        out =  self.relu1(out)\n",
    "        out =  self.fc2(out) \n",
    "\n",
    "\n",
    "        return out \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6124a8dc-f1d5-444a-8dcc-a82ab0429290",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf = SubWayNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d125918-1e01-460c-a34b-ecbd2bfe5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 =  nn.Conv2d(3,6,3,1)\n",
    "        self.conv2 =  nn.Conv2d(6,16,3,1)\n",
    "        self.fc1 = LazyLinear(120)\n",
    "        self.fc2= nn.Linear(120,84)\n",
    "        self.fc3 =  nn.Linear(84,2)\n",
    "        self.maxpool =  nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.maxpool2 =  nn.MaxPool2d(kernel_size=2,stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x =  self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x =  self.maxpool2(x)\n",
    "        x =  x.view(-1,54*54*16)\n",
    "        x =  self.relu(self.fc1(x))\n",
    "        x =  self.relu(self.fc2(x))\n",
    "        x =  self.fc3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d6e9ef-5726-4991-9f75-c41466885a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(1, 3, 32, 32).size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a55431-7dea-4c86-bd68-0266d6fad515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6ce8b3-3236-4701-a90b-53d5ced72871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ed2b83-1522-46d4-80f9-80fa969e1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = pic_dir+str(df_copy.image.iloc[3])+'.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311b85ba-165c-42c6-9a2e-d13996281538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(fn)\n",
    "# transform = transforms.ToTensor()\n",
    "# image_tensor = transform(image)\n",
    "\n",
    "# inverted_image_tensor = torch.flip(image_tensor, dims=[2])\n",
    "# inverted_image = transforms.ToPILImage()(inverted_image_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24388fde-7efd-4a37-96eb-d28407239102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoadImages(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file='testing.csv', transform=None):\n",
    "        self.root_dir =  \"SHOTS\" \n",
    "        self.re_map = {'a':\"left\", \"w\":\"jump\",\"s\":\"roll\",\"d\":\"right\",'blank':'None'}\n",
    "        \n",
    "        self.df=  pd.read_csv(csv_file,dtype={'image':int, 'keyPress':str})\n",
    "        \n",
    "        self.df['keyPress'] = self.df[\"keyPress\"].apply(self.cleaner)\n",
    "        self.df = self.df[self.df.image.apply(self.exists)]\n",
    "\n",
    "        # self.df['oneHot'] = torch.tensor(self.df['keyPress'].astype('category').cat.codes.values)\n",
    "        \n",
    "\n",
    "        self.df[\"keyPress\"] = self.df[\"keyPress\"].apply(lambda x: self.re_map.get(x))\n",
    "        \n",
    "        self.one_hot_encoded_df = pd.get_dummies(self.df['keyPress'], dtype=float)\n",
    "        self.df = pd.concat([self.df, self.one_hot_encoded_df], axis=1)\n",
    "\n",
    "        # self.transform = transform\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pt =  os.path.join(self.root_dir,str(self.df.iloc[idx][0])+\".jpg\")\n",
    "        img = Image.open(pt)\n",
    "        # y_label =  torch.tensor(self.df['oneHot'].iloc[idx])\n",
    "        y_label =  self.df.drop(columns=['image','keyPress']).iloc[idx]\n",
    "        # y_label =  y_label.view(-1)\n",
    "        # print(y_label)\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((int(img.size[0] / 2), int(img.size[1] / 2)))\n",
    "        ])\n",
    "        \n",
    "        # if self.transform:\n",
    "        img =  transform(img)\n",
    "\n",
    "        return img, torch.tensor(y_label).view(-1)\n",
    "        \n",
    "    def getMappings(self):\n",
    "        return dict(enumerate(self.df['keyPress'].astype('category').cat.categories))\n",
    "\n",
    "    def cleaner(self,x):\n",
    "        if 'blank' not in x:\n",
    "            \n",
    "            return  x[1]\n",
    "        else:\n",
    "            return 'blank'\n",
    "            \n",
    "            \n",
    "    def exists(self,filename:str):\n",
    "        if os.path.exists(f\"SHOTS/{filename}.jpg\"):\n",
    "            return True\n",
    "        else:\n",
    "        \n",
    "            return False\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db269cd4-73da-4272-8236-9f47c9942c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(filename:str):\n",
    "    \n",
    "    pt =  f\"SHOTS/{filename}.jpg\"\n",
    "    img = Image.open(pt)\n",
    "    \n",
    "        # y_label =  y_label.view(-1)\n",
    "        # print(y_label)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((int(img.size[0] / 2), int(img.size[1] / 2)))\n",
    "    ])\n",
    "        \n",
    "        # if self.transform:\n",
    "    return  transform(img).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4457e2bc-7468-4c74-92f8-21603c7eedb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'None', 1: 'jump', 2: 'left', 3: 'right', 4: 'roll'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom.getMappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ebef0a11-3fe6-490a-9705-a8b0126f1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 =  SubWayNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8ccf8ce-3122-43be-bf7e-003938072abe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'method'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aa \u001b[38;5;241m=\u001b[39m \u001b[43ma1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mvenv/ComputerVision/lib/python3.11/site-packages/torch/nn/modules/module.py:2104\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \n\u001b[1;32m   2071\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2106\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2107\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'method'>."
     ]
    }
   ],
   "source": [
    "aa = a1.load_state_dict(previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cee4b9b4-7d0e-49f8-8e1c-b0d10eb4b624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4901, 0.4862, 0.4843,  ..., 0.2353, 0.2353, 0.2353],\n",
       "          [0.4671, 0.4641, 0.4644,  ..., 0.2345, 0.2349, 0.2351],\n",
       "          [0.4618, 0.4617, 0.4644,  ..., 0.2205, 0.2300, 0.2358],\n",
       "          ...,\n",
       "          [0.1974, 0.1930, 0.2075,  ..., 0.4657, 0.4668, 0.4656],\n",
       "          [0.2266, 0.2284, 0.2283,  ..., 0.4611, 0.4632, 0.4608],\n",
       "          [0.1905, 0.2017, 0.1941,  ..., 0.3929, 0.3941, 0.3928]],\n",
       "\n",
       "         [[0.4544, 0.4555, 0.4564,  ..., 0.2549, 0.2549, 0.2549],\n",
       "          [0.4657, 0.4673, 0.4674,  ..., 0.2541, 0.2545, 0.2547],\n",
       "          [0.4683, 0.4683, 0.4675,  ..., 0.2401, 0.2496, 0.2554],\n",
       "          ...,\n",
       "          [0.1893, 0.1871, 0.1996,  ..., 0.1553, 0.1543, 0.1555],\n",
       "          [0.2237, 0.2284, 0.2257,  ..., 0.1562, 0.1546, 0.1564],\n",
       "          [0.2042, 0.2160, 0.2078,  ..., 0.1492, 0.1484, 0.1493]],\n",
       "\n",
       "         [[0.5059, 0.5059, 0.5059,  ..., 0.3294, 0.3294, 0.3294],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.3286, 0.3290, 0.3292],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.3146, 0.3242, 0.3299],\n",
       "          ...,\n",
       "          [0.2300, 0.2265, 0.2386,  ..., 0.1442, 0.1462, 0.1439],\n",
       "          [0.2813, 0.2832, 0.2806,  ..., 0.1562, 0.1589, 0.1558],\n",
       "          [0.2729, 0.2841, 0.2761,  ..., 0.1683, 0.1686, 0.1683]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cff14a60-9200-41ac-8855-a908cb923a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 =  prepare('600').to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(f1)\n",
    "torch.max(output,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b561a48d-b059-4d6f-940f-233d363fae8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.5712], device='cuda:0'),\n",
       "indices=tensor([0], device='cuda:0'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec81404d-204f-4a96-aedb-84b201aabec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Resize((int(original_size[0] / 2), int(original_size[1] / 2)))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d3c6b2-263d-4508-91aa-073a5960aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom =  LoadImages(csv_file=\"testing.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df938459-5070-4a1f-8481-bbbae1a0c46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LoadImages at 0x7facadd0fa90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc19aa2-52af-403b-a0da-4b5f830dfad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'None', 1: 'jump', 2: 'left', 3: 'right', 4: 'roll'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom.getMappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8cb109-7a64-4784-a781-d421e42f53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c98f2f6-ba41-4658-901d-6b9b276e7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size  =  int(0.8 * len(custom))\n",
    "test_size=  int(len(custom) - train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "669b9eb4-d055-451a-8e72-61ba43935041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset =  random_split(custom, [train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "496fdd46-b91c-406b-9d3c-c65e65df562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44930d09-13e0-4edb-b6de-4fb720b9920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = surf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63dc348b-24bc-41ab-979c-2501edcc8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict,'SUBWAY.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7be1ecc-6d7b-4bb1-b77f-e2c4669c11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "181caf1d-0989-4be1-8ac3-46c916418fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0  is 0.7300033122301102\n",
      "Cost at epoch 0  is 0.9479775810890859\n",
      "Cost at epoch 0  is 0.9009317611320992\n",
      "Cost at epoch 0  is 0.8842985322853656\n",
      "Cost at epoch 0  is 0.8693902261945375\n",
      "Cost at epoch 0  is 0.8674932275926639\n",
      "Cost at epoch 0  is 0.860658856750328\n",
      "Cost at epoch 0  is 0.859311437752422\n",
      "Cost at epoch 0  is 0.8568360245694754\n",
      "Cost at epoch 0  is 0.8472061750337432\n",
      "Cost at epoch 0  is 0.8446291833818257\n",
      "Cost at epoch 0  is 0.8472226218072584\n",
      "Cost at epoch 0  is 0.8447070659156097\n",
      "Cost at epoch 0  is 0.8443875903296617\n",
      "Cost at epoch 0  is 0.8434603071208514\n",
      "Cost at epoch 0  is 0.8458116807683875\n",
      "Cost at epoch 0  is 0.8464363238504907\n",
      "Cost at epoch 0  is 0.8416062122641207\n",
      "Cost at epoch 0  is 0.8411982548730794\n",
      "Cost at epoch 0  is 0.840356315759014\n",
      "Cost at epoch 0  is 0.8387774328651397\n",
      "Cost at epoch 0  is 0.8362983562768583\n"
     ]
    }
   ],
   "source": [
    "num_epoch =  1\n",
    "losses=[] \n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        \n",
    "        data =  data.to(device)\n",
    "        target = target.to(device)\n",
    "        # image = image.view(image.size(0), -1)\n",
    "        # feed forward \n",
    "        logits =  model(data)\n",
    "        loss =  criterion(logits,target)\n",
    "        \n",
    "        # store the losses\n",
    "        losses.append(loss.item()) \n",
    "\n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "        if batch_idx%100 ==0:\n",
    "            print(f'Cost at epoch {epoch}  is {sum(losses)/len(losses)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14b16561-0cbb-474a-9329-ed8bc820323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous =  torch.load(\"SUBWAY.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42f13fb8-3c36-4d3f-afa2-d7e767f4073f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of SubWayNet(\n",
       "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_layer2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_layer3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_layer4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1720320, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efdfaf-4294-4cbf-962b-8283f8ade628",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape, j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7a71d-e767-4cfa-9875-bc00b293b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_x = losses[::int(len(losses)/200)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea2a89-b1f1-4fd7-9be5-b6ada410ce1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(200), downsampled_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb14e4-6972-4dbf-a460-a90e87bff31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e47c8e48-ee08-4107-b7cd-e9a1a15ccb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples=0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x= x.to(device)\n",
    "            y= y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions =  scores.max(1)\n",
    "            num_correct += (predictions==y.view(-1,1)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/ float(num_samples) *100}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76842cba-3d2d-4a00-a24b-8a769d810cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 138640 / 4334 with accuracy 3198.8924780802954\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef2bd047-7d6a-4c94-86ca-e88e3f170a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 640.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables to store total correct predictions and total samples\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Iterate over the validation dataset\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            inputs,labels =  inputs.to(device), labels.to(device).view(-1,1)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get the predicted class labels\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Compute the number of correct predictions in the current batch\n",
    "            batch_correct = (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update total correct predictions and total samples\n",
    "            total_correct += batch_correct\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    # Convert accuracy to percentage\n",
    "    accuracy_percentage = accuracy * 100\n",
    "    \n",
    "    return accuracy_percentage\n",
    "\n",
    "# Assuming `model` is your PyTorch model\n",
    "# Assuming `valid_loader` is your data loader for the validation set\n",
    "\n",
    "# Calculate the accuracy of the model on the validation set\n",
    "accuracy_percentage = calculate_accuracy(model, train_loader)\n",
    "\n",
    "print('Validation Accuracy:', accuracy_percentage, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5037f43d-34a1-4c1d-92ef-3ce8b83bb124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_percentage/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dbb6a8c-43a4-4b47-9c4b-efa7fa92c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = model(i.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d452f70-93c1-4ecd-b5dd-7f94313cb557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "971d8dce-5803-42fa-85e4-f30db423e6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa84ccbe-24fe-4acd-914a-ecc3db0281e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.to(device).view(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "743551eb-1796-46c9-b603-cf0aa475acaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred==j.to(device).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38551a-9d56-4fd9-a2a9-a627c0b0c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " x.view( -1, x.size( 1 )).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c7115-9064-4f31-91fb-ddef6c216a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = model(x.to(device))\n",
    "sc.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8681f0-6348-4226-b01c-031b2cc624ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a16717-70d3-48ea-8695-33a2922757c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337bd30-0387-43c6-a5c8-063833f530dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(surf.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bf12e-7b11-441c-8f18-ba429e714c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=  pd.read_csv(\"testing.csv\",dtype={'image':int, 'keyPress':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc00b6c-0270-450a-b69f-026b3ba29e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # # self.re_map = {'a':\"left\", \"w\":\"jump\",\"s\":\"roll\",\"d\":\"right\",'blank':'None'}\n",
    "        \n",
    " #        self.df=  pd.read_csv(csv_file,dtype={'image':int, 'keyPress':str})\n",
    "        \n",
    " #        self.df['keyPress'] = self.df[\"keyPress\"].apply(self.cleaner)\n",
    " #        self.df = self.df[~self.df.image.apply(self.exists).isna()]\n",
    "\n",
    " #        self.df['oneHot'] = torch.tensor(self.df['keyPress'].astype('category').cat.codes.values)\n",
    "\n",
    " #        self.df[\"keyPress\"] = self.df[\"keyPress\"].apply(lambda x: self.re_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220d357-25d0-4ec7-8b81-025be7a4a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(x):\n",
    "    if 'blank' not in x:\n",
    "        return  x[1]\n",
    "    else:\n",
    "        return 'blank'\n",
    "            # \n",
    "def exists(filename:str):\n",
    "    if os.path.exists(f\"SHOTS/{filename}.jpg\"):\n",
    "        return True\n",
    "    else:\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e14fc-447d-4aaa-a757-4d72f7ba5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"testing.csv\",dtype={'image':int, 'keyPress':str})\n",
    "# df['keyPress']  = df[\"keyPress\"].apply(cleaner)\n",
    "ss = df.image.apply(exists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fcb0e-4c27-4a8f-8b9e-4813512f14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(f\"SHOTS/{df.image.iloc[1]}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcf35d-4151-412f-8ddd-ff498d4b013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image.apply(exists)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcadf89-c892-4493-87b0-5987c183731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef7008-faa8-41ab-9903-4b24308dd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "exists('2708.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebe6c5-528e-4b4c-bbce-b6a9eaf5c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd44970-fc73-46e1-a4a0-5fb43481a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoded_df = pd.get_dummies(df['keyPress'], dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658deee1-05c0-4dc5-8ac4-1736a176bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318c955-7f5a-4a22-bfb2-5e2acf7ba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded = pd.concat([df, one_hot_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4b141-51b5-4db7-bdf2-43f1080ba1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp = df_encoded.drop(columns='keyPress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976610e8-bd21-4509-b5ba-b0809ae3e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded[['a','blank','d','s','w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d730a9d-77dd-4e72-b1a7-a3ef0d74d472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
